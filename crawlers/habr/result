{
    "habr.com": {
        "1": {
            "title": "Как я переписывал поисковик авиабилетов с PHP на NodeJS",
            "URL": "https://habr.com/en/post/444400/",
            "author": "JamesJGoodwin",
            "data_published": "2019-03-19T12:11:10.000Z",
            "text": "Привет. Меня зовут Андрей, я студент-магистрант в одном из технических ВУЗов Москвы и по совместительству очень скромный начинающий предприниматель и разработчик. В этой статье я решил поделиться своим опытом перехода от PHP (который когда-то мне нравился из-за своей простоты, но со временем стал ненавидим мною — под катом объясняю почему) к NodeJS. Здесь могут приводиться очень банальные и кажущиеся элементарными задачи, которые, тем не менее, лично мне было любопытно решать в ходе моего знакомства с NodeJS и особенностями серверной разработки на JavaScript. Я попытаюсь объяснить и наглядно доказать, что PHP уже окончательно ушёл в закат и уступил своё место NodeJS. Возможно, кому-то даже будет полезно узнать некоторые особенности рендеринга HTML-страниц в Node, который изначально не приспособлен к этому от слова совсем. Введение Во время написания движка я использовал простейшие техники. Никаких менеджеров пакетов, никакого роутинга. Только хардкор — папки, чьё название соответствовало запрашиваемому маршруту, да index.php в каждой из них, настроенный PHP-FPM для поддержания пула процессов. Позднее возникла необходимость использовать Composer и Laravel, что и стало последней каплей для меня. Перед тем, как перейти к рассказу о том, зачем же я вообще взялся переписывать всё с PHP на NodeJS, расскажу немного о предыстории. Менеджер пакетов В конце 2018 года довелось работать с одним проектом, написанным на Laravel. Нужно было исправить несколько багов, внести правки в существующий функционал, добавить несколько новых кнопок в интерфейсе. Начался процесс с установки менеджера пакетов и зависимостей. В PHP для этого используется Composer. Тогда заказчик предоставил сервер с 1 ядром и 512 мегабайтами оперативной памяти и это был мой первый опыт работы с Composer. При установке зависимостей на виртуальном приватном сервере с 512 мегабайтами памяти процесс аварийно завершился из-за нехватки памяти.  Для меня, как человека знакомого с Linux и имеющего опыт работы с Debian и Ubuntu решение такой проблемы было очевидно — установка SWAP-файла (файл подкачки — для тех, кто не знаком с администрированием в Linux). Начинающий неопытный разработчик, установивший свой первый Laravel-дистрибутив на Digital Ocean, например, просто пойдёт в панель управления и будет поднимать тариф до тех пор, пока установка зависимостей не перестанет завершаться с ошибкой сегментации памяти. А как там обстоят дела с NodeJS?\r А в NodeJS есть свой собственный менеджер пакетов — npm. Он значительно проще в использовании, компактнее, может работать даже в среде с минимальным количеством оперативной памяти. В целом, ругать Composer на фоне NPM особо не за что, однако в случае возникновения каких-то ошибок при установке пакетов Composer вылетит с ошибкой как обычное PHP-приложение и вы никогда не узнаете, какая часть пакета успела установиться и установилась ли она в конце-концов. И вообще, для администратора Linux вылетевшая установка = флэшбеки в Rescue Mode и dpkg --configure -a. К тому времени, как меня настигли такие \"сюрпризы\", я уже недолюбливал PHP, но это уже были последние гвозди в крышку гроба моей некогда великой любви к PHP. Long-term support и проблема версионирования Помните, какой ажиотаж и изумление вызвал PHP7, когда разработчики впервые презентовали его? Увеличение производительности более чем в 2 раза, а в некоторых компонентах до 5 раз! А помните, когда появился на свет PHP седьмой версии? А как быстро-то заработал WordPress! Это был декабрь 2015 года. А знали ли вы, что PHP 7.0 уже сейчас считается устаревшей версией PHP и крайне рекомендуется обновить его… Нет, не до версии 7.1, а до версии 7.2. По утверждениям разработчиков, версия 7.1 уже лишена активной поддержки и получает лишь обновления безопасности. А через 8 месяцев прекратится и это. Прекратится, вместе с активной поддержкой и версии 7.2. Выходит, что к концу текущего года у PHP останется лишь одна актуальная версия — 7.3.  На самом деле, это не было бы придиркой и я бы не отнёс это к причинам моего ухода от PHP, если бы написанные мной проекты на PHP 7.0.* уже сейчас не вызывали deprecation warning при открытии. Вернёмся к тому проекту, где вылетала установка зависимостей. Это был проект, написанный в 2015 году на Laravel 4 с PHP 5.6. Казалось, прошло же всего 4 года, ан-нет — куча deprecation-предупреждений, устаревшие модули, невозможность нормально обновиться до Laravel 5 из-за кучи корневых обновлений движка. И это касается не только самого Laravel. Попробуйте взять любое приложение на PHP, написанное во времена активной поддержки первых версий PHP 7.0 и будьте готовы потратить свой вечер на поиск решений проблем, возникших в устаревших модулях PHP. Напоследок интересный факт: поддержка PHP 7.0 была прекращена раньше, чем поддержка PHP 5.6. На секундочку. А как дела с этим обстоят у NodeJS? Я бы не сказал, что здесь всё значительно лучше и что сроки поддержки у NodeJS кардинально отличаются от PHP. Нет, здесь всё примерно так же — каждая LTS-версия поддерживается в течение 3 лет. Вот только у NodeJS немного больше этих самых актуальных версий.  Если у вас возникнет необходимость развернуть приложение, написанное в 2016 году, то будьте уверены, что у вас не будет с этим совершенно никаких проблем. Кстати говоря, версия 6.* перестанет поддерживаться только в апреле этого года. А впереди ещё 8, 10, 11 и готовящаяся 12. О трудностях и сюрпризах при переходе на NodeJS Начну, пожалуй, с самого волнующего меня вопроса о том, как же всё-таки рендерить HTML-страницы в NodeJS. Но давайте для начала вспомним, как это делается в PHP:  Встраивать HTML напрямую в PHP-код. Так делают все новички, кто ещё не добрался до MVC. И так сделано в WordPress, что категорически ужасающе. Использовать MVC, что как бы должно упростить взаимодействие разработчика и обеспечить какое-то разбиение проекта на части, но на деле этот подход лишь усложняет всё в разы. Использовать шаблонизатор. Наиболее удобный вариант, но не в PHP. Просто посмотрите синтаксис, предлагаемый в Twig или Blade с фигурными скобками и процентами.  Я являюсь ярым противником совмещения или слияния нескольких технологий воедино. HTML должен существовать отдельно, стили для него — отдельно, JavaScript-отдельно (в React это вообще выглядит чудовищно — HTML и JavaScript вперемешку). Именно поэтому идеальный вариант для разработчиков с предпочтениями как у меня — шаблонизатор. Искать его для веб-приложения на NodeJS долго не пришлось и я сделал выбор в пользу Jade (PugJS). Просто оцените простоту его синтаксиса:     div.row.links         div.col-lg-3.col-md-3.col-sm-4             h4.footer-heading Флот.ру             div.copyright                 div.copy-text 2017 - #{current_year}  Флот.ру                 div.contact-link                     span Контакты:                         a(href='mailto:hello@flaut.ru') hello@flaut.ru Здесь всё достаточно просто: написал шаблон, загрузил его в приложение, скомпилировал один раз и далее используешь в любом удобном месте в любое удобное время. По моим ощущениям, производительность PugJS примерно в 2 раза лучше, чем рендеринг с помощью внедрения HTML в PHP-код. Если раньше в PHP статическая страница генерировалась сервером примерно за 200-250 миллисекунд, то теперь это время составляет примерно 90-120 миллисекунд (речь идёт не о самом рендеринге в PugJS, а о времени, затраченном от запроса страницы до ответа сервера клиенту с готовым HTML). Так выглядит загрузка и компиляция шаблонов и их компонентов на стадии запуска приложения: const pugs = {}  fs.readdirSync(__dirname + '/templates/').forEach(file => {     if(file.endsWith('.pug')) {         try {             var filepath = __dirname + '/templates/' + file             pugs[file.split('.pug')[0]] = pug.compile(fs.readFileSync(filepath, 'utf-8'), { filename: filepath })         } catch(e) {             console.error(e)         }     } })  // и далее рендеринг уже скомпилированного шаблона return pugs.tickets({  ...config }) Выглядит невероятно просто, но с Jade возникла небольшая сложность на этапе работы с уже скомпилированным HTML. Дело в том, что для внедрения скриптов на странице используется асинхронная функция, которая забирает все .js файлы из директории и добавляет к каждому из них дату их последнего изменения. Функция имеет следующий вид: for(let i = 0; i < files.length; i++) {     let period = files[i].lastIndexOf('.') // get last dot in filename     let filename = files[i].substring(0, period)     let extension = files[i].substring(period + 1)      if(extension === 'js') {         let fullFilename = filename + '.' + extension         if(env === 'production') {             scripts.push({ path: paths.production.web + fullFilename, mtime: await getMtime(paths.production.code + fullFilename)})         } else {             if(files[i].startsWith('common') || files[i].startsWith('search')) {                 scripts.push({ path: paths.developer.scripts.web + fullFilename, mtime: await getMtime(paths.developer.scripts.code + fullFilename)})             } else {                 scripts.push({ path: paths.developer.vendor.web + fullFilename, mtime: await getMtime(paths.developer.vendor.code + fullFilename)})             }         }     } } На выходе получаем массив объектов с двумя свойствами — путь к файлу и время его последнего редактирования в timestamp (для обновления клиентского кэша). Проблема в том, что ещё на этапе сбора файлов скриптов из директории они все загружаются в память строго по алфавиту (так они расположены в самой директории, а сбор файлов в ней осуществляется сверху вниз — от первого до последнего). Это приводило к тому, что сначала загружался файл app.js, а уже после него шёл файл core.min.js с полифиллами, и в самом конце — vendor.min.js. Решилась эта проблема достаточно просто — очень банальной сортировкой: scripts.sort((a, b) => {     if(a.path.includes('core.min.js')) {         return -1     } else if(a.path.includes('vendor.min.js')) {         return 0     }     return 1 }) В PHP же это всё имело монструозный вид в виде заранее записанных в строку путей к JS-файлам. Просто, но непрактично. NodeJS держит своё приложение в оперативной памяти Это огромный плюс. У меня всё устроено так, что на сервере параллельно и независимо друг от друга существуют два отдельных сайта — версия для разработчика и production-версия. Представим, что я сделал какие-то изменения в PHP-файлы на сайте для разработки и мне нужно выкатить эти изменения на production. Для этого нужно останавливать сервер или ставить заглушку \"извините, тех. работы\" и в это время копировать файлы по отдельности из папки developer в папку production. Это вызывает какой-никакой простой и может вылиться в потерю конверсий. Преимущество in-memory application в NodeJS заключается для меня в том, что все изменения в файлы движка будут внесены только после его перезагрузки. Это очень удобно, поскольку можно скопировать все необходимые файлы с изменениями и только после этого перезагрузить сервер. Процесс занимает не более 1-2 секунд и не вызовет даунтайма.\r Такой же подход используется в nginx, например. Вы сначала редактируете конфигурацию, проверяете её при помощи nginx -t и только потом вносите изменения с service nginx reload Кластеризация NodeJS-приложения В NodeJS есть очень удобный инструмент — менеджер процессов pm2. Как мы обычно запускаем приложения в Node? Заходим в консоль и пишем node index.js. Как только мы закрываем консоль — приложение закрывается. По крайней мере, так происходит на сервере с Ubuntu. Чтобы избежать этого и держать приложение запущенным всегда, достаточно добавить его в pm2 простой командой pm2 start index.js --name production. Но это ещё не всё. Инструмент позволяет проводить мониторинг (pm2 monit) и кластеризацию приложения. Давайте вспомним о том, как организованы процессы в PHP. Допустим, у нас есть nginx, обслуживающий http-запросы и нам нужно передать запрос на PHP. Можно либо сделать это напрямую и тогда при каждом запросе будет спавниться новый процесс PHP, а при его завершении — убиваться. А можно использовать fastcgi-сервер. Думаю, все знают что это такое и нет необходимости вдаваться в подробности, но на всякий случай уточню, что в качестве fastcgi чаще всего используется PHP-FPM и его задача — заспавнить множество процессов PHP, которые в любой момент готовы принять и обработать новый запрос. В чём недостаток такого подхода?  Во-первых, в том, что вы никогда не знаете, сколько памяти будет потреблять ваше приложение. Во-вторых, вы всегда будете ограничены в максимальном количестве процессов, а соответственно, при резком скачке трафика ваше PHP-приложение либо использует всю доступную память и упадёт, либо упрётся в допустимый предел процессов и начнёт убивать старые. Это можно предотвратить, установив не-помню-какой-параметр в конфигурационном файле PHP-FPM в dynamic и тогда будет спавниться столько процессов, сколько необходимо в данное время. Но снова-таки, элементарная DDoS-атака выест всю оперативную память и положит ваш сервер. Или, например, багнутый скрипт съест всю оперативную память и сервер зависнет на какое-то время (были прецеденты в процессе разработки). Кардинальное отличие в NodeJS заключается в том, что приложение не может потребить более чем 1,5 гигабайта оперативной памяти. Нет никаких ограничений по процессам, есть только ограничение по памяти. Это стимулирует писать максимально легковесные программы. К тому же очень просто рассчитать количество кластеров, которые мы можем себе позволить в зависимости от имеющегося ресурса CPU. Рекомендуется на каждое ядро \"вешать\" не более одного кластера (ровно как и в nginx — не более одного воркера на одно ядро CPU).  Преимущество такого подхода ещё и в том, что PM2 перезагружает все кластеры по очереди. Возвращаясь к предыдущему абзацу, в котором говорилось о 1-2 секундном даунтайме при перезагрузке. В Cluster-Mode при перезагрузке сервера ваше приложение не испытает ни миллисекунды даунтайма. NodeJS — это хороший швейцарский нож Ныне сложилась такая ситуация, когда PHP выступает в качестве языка для написания сайтов, а Python выступает в качестве инструмента для краулинга этих самых сайтов. NodeJS — это 2 в 1, с одной стороны вилка, с другой — ложка. Вы можете писать быстрые и производительные приложения и веб-краулеры на одном сервере в пределах одного приложения. Звучит заманчиво. Но как это может быть реализовано, спросите вы? Сама компания Google выкатила официальное API с Chromium — Puppeteer. Вы можете запускать Headless Chrome (браузер без интерфейса пользователя — \"безголовый\" Chrome) и получить максимально широкий доступ к API-браузера для обхода страниц. Максимально просто и доступно о работе Puppeteer. Например, в нашей группе ВКонтакте происходит регулярный постинг скидок и специальных предложений на различные направления из городов СНГ. Мы генерируем изображения для постов в автоматическом режиме, а чтобы они были ещё и красивыми — нам нужны красивые картинки. Я не любитель подвязываться на различные API и заводить на десятках сайтов аккаунты, поэтому я написал простое приложение, имитирующее обычного пользователя с браузером Google Chrome, который ходит по сайту со стоковыми картинками и забирает случайным образом изображение, найденное по ключевому слову. Раньше для этого я использовал Python и BeautifulSoup, но теперь в этом отпала необходимость. А самая главная особенность и преимущество Puppeteer заключается в том, что вы можете с лёгкостью краулить даже SPA-сайты, ведь вы имеете в своём распоряжении полноценный браузер, понимающий и исполняющий JavaScript-код на сайтах. Это до боли просто: const browser = await puppeteer.launch({headless: true, args:['--no-sandbox']}) const page = (await browser.pages())[0] await page.goto(`https://pixabay.com/photos/search/${imageKeyword}/?cat=buildings&orientation=horizontal`, { waitUntil: 'networkidle0' }) Вот так в 3 строчки кода мы запустили браузер и открыли страницу сайта со стоковыми изображением. Теперь мы можем выбрать случайный блок с изображением на странице и добавить к нему класс, по которому в дальнейшем сможем точно так же обратиться и уже перейти на страницу непосредственно с самим изображением для его дальнейшей загрузки: var imagesLength = await page.evaluate(() => {     var photos = document.querySelectorAll('.search_results > .item')      if(photos.length > 0) {         photos[Math.floor(Math.random() * photos.length)].className += ' --anomaly_selected'     }      return photos.length }) Вспомните, сколько бы потребовалось кода, чтобы написать подобное на PhantomJS (который, кстати, закрылся и вступил в тесное сотрудничество с командой разработки Puppeteer). Разве наличие такого чудесного инструмента может остановить кого-нибудь от перехода к NodeJS? В NodeJS заложена асинхронность на фундаментальном уровне Это можно считать огромным преимуществом NodeJS и JavaScript, особенно с приходом async/await в ES2017. В отличии от PHP, где любой вызов выполняется синхронно. Приведу простой пример. Раньше у нас в поисковике страницы генерировались на сервере, но кое-что нужно было выводить на страницу уже в клиенте с помощью JavaScript, а на тот момент Яндекс ещё не умел в JavaScript на сайтах и специально для него пришлось реализовывать механизм снапшотов (снимков страниц) при помощи Prerender. Снапшоты хранились у нас на сервере и выдавались роботу при запросе. Дилемма заключалась в том, что эти снимки генерировались в течение 3-5 секунд, что совершенно недопустимо и может повлиять на ранжирование сайта в поисковой выдаче. Для решения этой задачи был придуман простой алгоритм: когда робот запрашивает какую-то страницу, снимок которой у нас уже есть, то мы просто отдаём ему уже имеющийся у нас снимок, после чего \"в фоне\" выполняем операцию по созданию нового снимка и заменяем им уже имеющийся. Как это было делано в PHP: exec('/usr/bin/php ' . __DIR__ . '/snapshot.php -a ' . $affiliation_type . ' -l ' . urlencode($full_uri) . ' > /dev/null 2>/dev/null &'); Никогда так не делайте.\r В NodeJS этого можно добиться вызовом асинхронной функции: async function saveSnapshot() {     getSnapshot().then((res) => {         db.saveSnapshot().then((status) => {             if(status.err) console.error(err)         })     }) }  /**  * И вызываем функцию без await  * Т.е. не будем дожидаться resolve() от промиса  */  saveSnapshot() Вкратце, вы не пытаетесь обойти синхронность, а сами определяете, когда использовать синхронное выполнение кода, а когда асинхронное. И это действительно удобно. Особенно когда вы узнаете о возможностях Promise.all() Сам движок поисковика авиабилетов устроен таким образом, что он отсылает запрос на второй сервер, собирающий и агрегирующий данные, и затем обращается к нему за уже готовыми к выдаче данными. Для привлечения трафика из органики используются страницы направлений.  Например, по запросу \"Авиабилеты Москва Санкт-Петербург\" будет выдана страница с адресом /tickets/moscow/saint-petersburg/, а на ней нужны данные:  Цены авиабилетов по этому направлению на текущий месяц Цены авиабилетов по этому направлению на год вперёд (средняя цена по каждому месяцу для следующих 12 месяцев) Расписание авиарейсов по этому направлению Популярные направления из города отправки — из Москвы (для перелинковки) Популярные направления из города прибытия — из Санкт-Петербурга (для перелинковки)  В PHP все эти запросы выполнялись синхронно — один за другим. Среднее время ответа API по одному запросу — 150-200 миллисекунд. Умножаем 200 на 5 и получаем в среднем секунду только на выполнение запросов к серверу с данными. В NodeJS есть замечательная функция Promise.all, которая выполняет все запросы параллельно, но записывает результат поочерёдно. Например, так выглядел бы код выполнения всех пяти выше описанных запросов: var [montlyPrices, yearlyPrices, flightsSchedule, originPopulars, destPopulars] = await Promise.all([     getMontlyPrices(),     getYearlyPrices(),     getFlightSchedule(),     getOriginPopulars(),     getDestPopulars() ]) И получаем все данные за 200-300 миллисекунд, сократив время генерации данных для страницы с 1-1,5 секунд до ~500 миллисекунд. Заключение Переход с PHP на NodeJS помог мне ближе познакомиться с асинхронным JavaScript, научиться работе с промисами и async/await. После того, как движок был переписан, скорость загрузки страниц была оптимизирована и отличалась разительно от результатов, которые показывал движок на PHP. В этой статье можно было бы ещё рассказать о том, насколько просто используются модули для работы с кэшем (Redis) и pg-promise (PostgreSQL) в NodeJS и устроить сравнение их с Memcached и php-pgsql, но эта статья итак получилась достаточно объёмной. А зная мой \"талант\" к писательству, она получилась ещё и плохо структурированной. Цель написания этой статьи — привлечь внимание разработчиков, которые до сих пор работают с PHP и не знают о прелестях NodeJS и разработки веб-ориентированных приложений на нём на примере реально существующего проекта, который когда-то был написан на PHP, но из-за предпочтений своего владельца ушёл на другую платформу. Надеюсь, что мне удалось донести мои мысли и более-менее структурировано изложить их в этом материале. По крайней мере, я старался :) Пишите любые комментарии — доброжелательные или гневные. Буду отвечать на любой конструктив. "
        },
        "2": {
            "title": "Tactoom.com изнутри — социальная блог-платформа на NodeJS/NoSQL",
            "URL": "https://habr.com/en/post/130345/",
            "author": "octave",
            "data_published": "2011-10-14T19:08:32.000Z",
            "text": "Итак, пришло время раскрыть некоторые карты и рассказать о том, как устроен Tactoom изнутри. \r В этой статье я расскажу о разработке и выведении в production веб-сервиса с использованием:\r NodeJS (fibers), MongoDB, Redis, ElasticSearch, Capistrano, Rackspace.   Вступление\r Три недели назад мы с Давидом (DMiloshev) запустили инфосоциальную сеть Tactoom.com. О том, что это такое можно почитать здесь. \r На фоне шума, недавно поднятого вокруг NodeJS, вероятно, многим интересно, что же эта технология представляет из себя не на словах, а на деле. \r NodeJS — это вовсе не панацея. Это просто еще одна технология, по сути, ничем не лучше других. Для того чтобы добиться хорошей производительности и масштабируемости, вам придется хорошенько попотеть — так же, как и везде.  Архитектура приложения\r NodeJS приложение делится на два вида процессов:\r 1. Web процесс (http)\r 2. Cloud процесс (очереди) \r Все процессы абсолютно независимы друг от друга, могут находиться на разных серверах и даже в разных точках земного шара. При этом, масштабируется приложение как раз при помощи мультипликации этих процессов. Общение между ними происходит сугубо через централизированный сервер сообщений (redis).  Web процессы обслуживают прямые http запросы пользователей. Каждый процесс может обрабатывать множество запросов одновременно. Учитывая специфику Eventloop, в зависимости от соотношения CPU/IO каждого конкретного запроса, предел параллельной обработки может либо снижаться, либо повышаться для отдельного процесса на момент времени.  Cloud процессы производят операции, которые не связаны напрямую с пользовательскими запросами. Например: отправка email-ов, денормализация данных, поисковая индексация. Как и Web, один Cloud процесс может обрабатывать множество задач разных типов одновременно.\r Стоит отметить, что здесь очень важна «атомарность» задач/запросов. То есть, нужно следить за тем, чтобы емкая задача/вычисление было разбито на множество более мелких частей, которые затем будут равномерно распределены по остальным процессам. Это повысит скорость выполнения задачи, отказоустойчивость и снизит потребление памяти и коэффициент блокировки каждого процесса и сервера целиком.  Web → Cloud\r Я стараюсь организовать Web процессы таким образом, чтобы повысить общий коэффициент времени IO против CPU, а значит сфокусировать их на быстрой выдаче http при высокой конкурентности запросов. Это значит, что Web делегирует high-cpu логику в Cloud, ждет ее выполнения, затем получает результат вычислений. Соответственно, в силу асинхронной архитектуры nodejs, во время ожидания Web может выполнять другие запросы.  Кластеризация\r Архитектура Web и Cloud очень схожа, за исключением того, что вместо http сокета Cloud «слушает» redis очередь. \r Кластеризация node процессов происходит по следующим принципам:\r 1. На каждом физическом сервере запущен один процесс-супервизор (node-cluster)\r 2. Дочерние процессы супервизора и есть наши Web-ы и Cloud-ы, количество которых всегда равно количеству ядер сервера.\r 3. Супервизор контролирует потребление памяти каждым дочерним процессом и, в случае превышения заданной нормы, перезапускает его (предварительно дождавшись завершения текущих запросов этого процесса).    Fibers\r Весь высокоуровневый слой приложения написан с использованием node-sync (fibers), без которого я вообще слабо себе представляю его разработку. Дело в том, что такие сложные вещи, как та же сборка статики, реализовать на «официальной» callback-driven парадигме весьма сложно, если не глупо. Тем, кто еще не видел код того же npm, настоятельно рекомендую на него посмотреть, и попытаться понять, что там происходит, а главное — зачем. А холивары и троллинг, которые разрастаются вокруг асинхронной парадигмы nodejs чуть ли не каждый день, мягко говоря, вызывают у меня недоумение. \r Подробнее о node-sync вы можете узнать в моей статье: node-sync — псевдо-синхронное программирование на nodejs с использованием fibers  Web\r Общая логика Web приложения реализована на фреймворке expressjs в стиле «express». За исключением того, что каждый запрос заворачивается в отдельный Fiber, внутри которого все операции выполняются в синхронном стиле. \r В силу невозможности переопределить некоторые участки функциональности expressjs, в частности роутинга, его пришлось вынести из npm и включить в основной репозиторий проекта. Это же касается и ряда других модулей (особенно тех, которые разработаны LearnBoost), ибо contributing в их проекты дается очень большим трудом и вообще не всегда удается. \r CSS генерируется через stylus. Это действительно очень удобно.\r Шаблонизатор — ejs (как на сервере, так и на клиенте).\r Загрузка файлов — connect-form. \r Web работает очень быстро, поскольку все модули и инициализация загружаются в память процесса при запуске. Я стараюсь держать среднее время отклика Web процесса на любую страницу — до 300ms (исключая загрузку изображений, регистрацию и т.д.). Проводя профайлинг, я с удивлением обнаружил, что 70% этого времени отнимает работа mongoose (mongodb ORM для nodejs) — об этом ниже.  i18n\r Долго искал подходящее решение для интернационализации в nodejs, и мои поиски сошлись на node-gettext с небольшим допиливанием. Работает как часы, файлы локалей подтягиваются «на лету» серверными nodejs процессами при обновлении.  Кэш\r Функциональность кэширования со всей своей логикой уместилась в два экрана кода. В качестве cache-backend используется redis.  Память\r У Web процессов память течет рекой, как позднее оказалось, из-за mongoose. Один процесс (днем, во время средней нагрузки), съедает до 800MB за два часа, после чего перезапускается супервизором.\r Утечки памяти в nodejs искать довольно сложно, если вы знаете интересные способы — дайте мне знать.  Данные\r Как показала практика, schema-less парадигма mongodb идеально подходит для модели Tactoom. Сама БД ведет себя хорошо (весит 376MB, из них 122MB — индекс), данные выбираются исключительно по индексам, поэтому результат любого запроса — не больше 30ms, даже при высокой нагрузке (большинство запросов вообще <1ms). \r Если интересно, во второй части могу более подробно рассказать о том, как удалось «приручить» mongodb для ряда нетривиальных задач (и как не удалось).  mongoosejs (mongodb ORM для nodejs)\r О нем хочу отдельно сказать. Выбор списка 20-ти пользователей: запрос и выбор данных в mongo занимает 2ms, передача данных — 10ms, потом mongoose делает что-то еще 200ms (уже молчу про память) и в итоге получаю объекты. Если переписать это на более низкоуровневый node-mongodb-native, то все это займет 30ms.\r Постепенно, мне пришлось переписать практически все на mongodb-native, при этом, повысив быстродействие системы в целом раз в 10.  Статика\r Вся статика Tactoom хранится на Rackspace Cloud Storage. При этом я использую статический домен cdnX.infosocial.net, где X — 1..n. Этот домен пробрасывает через DNS на внутренний домен контейнера в Cloud Storage, позволяя браузерам грузить статические файлы параллельно. Каждый статический файл хранится в двух копиях (plain и gzip) и имеет уникальное имя, в которое зашита версия. Если версия файла обновится — адрес изменится, и браузеры загрузят новый файл.  Сборка статики приложения (клиентский js и css, картинки) происходит через самописный механизм, который определяет измененные файлы (через git-log), делает minify, делает gzip копию и загружает их на CDN. Скрипт сборки так же следит за измененными изображениями и обновляет их адреса в соответствующих css файлах.\r Список (mapping) статики адресов всех файлов хранится в Redis. Этот список загружает в память каждый Web процесс при запуске либо при обновлении статических версий.\r По сути, дэплоймент любых изменений статики осуществляется одной командой, которая делает все сама. Причем это не требует никакой перезагрузки, поскольку nodejs приложения подхватывают измененные адреса статических файлов на лету через redis pub/sub.  Пользовательская статика тоже хранится на Rackspace, но в отличие от статики приложения, не имеет версий, а просто проходит определенную канонизацию, позволяющую по хэшу картинки получить адреса всех ее размеров на CDN. \r Для определений хоста (cdnX), на котором хранится конкретный статический файл, используется consistent hashing.  Серверная архитектура  \r По сути, Tactoom раскидан на 3 гермозоны: 1. Rackspace — площадка для быстрого масштабирования и хранения статики  2. Площадка в Европе — здесь наши физические сервера 3. Секрет (сюда ротэйтятся логи, производятся фоновые вычисления и собирается статистика) \r В мир смотрит только один сервер — nginx, с открытыми портами 80 и 4000. Последний используется для COMET соединений.\r Остальные сервера общаются между собой по прямым ip, закрыты от мира через iptables.  :80\r nginx проксирует запросы через upstream конфигурацию на Web серверы. На данный момент есть два апстрима: tac_main и tac_media. Каждый из них содержит список Web серверов, на которых работает node-cluster по 3000 порту, у каждого Web сервера есть свой приоритет при распределении запросов. tac_main — кластер Web серверов, которые находятся близко к базе данных и отвечают за выдачу большинства веб-страниц для зарегистрированных пользователей Tactoom. tac_media — кластер Web серверов, находящихся близко к CDN. Через них происходят все операции по загрузке и ресайзингу изображений. \r Сервера webN и cloudN изображены, чтобы показать, где я добавляю сервера при хабраэффекте и других приятных событиях.\r Новые сервера поднимаются в течение 10 минут — по образу, сохраненному на CDN.  :4000\r Тут обычный proxy-pass на сервер comet, где работает nodejs COMET приложение Beseda, о которой я расскажу во второй части.  tac1, tac2, data1\r Это главные сервера Tactoom: XEON X3440 4x2.53 ГГц 16 ГБ 2x1500 ГБ Raid1.\r На каждом работает Mongod процесс, все они объединены в ReplicaSet с автоматическим failover-ом и дистрибьюцией операций чтения на slave-ы. \r На tac1 — главный Web кластер, на tac2 — Cloud кластер. В каждом кластере по 8 nodejs процессов. \r В ближайшее время создам еще один upstream tac_search, на который будут роутиться исключительно поисковые запросы. В нем будет Web-cluster, который поставлю рядом с elasticsearch (про него во второй части) сервером.  Выводы\r Цитируя лозунг создателей NodeJS: «Because nothing blocks, less-than-expert programmers are able to develop fast systems.» «Из-за того, что ничего не блокируется, менее-чем-эксперты могут разрабатывать быстрые системы.»   Это обман. Я использую nodejs уже почти 2 года и на собственном опыте знаю, что для того, чтобы на нем разрабатывать «быстрые системы», нужно не меньше опыта (а то и больше), чем на любой другой технологии. В реальности же с callback-driven парадигмой в nodejs и особенностями javascript в целом, скорее легче сделать ошибку (и потом очень долго ее искать), чем выиграть в производительности.  С другой стороны, троллинг господина Ted Dziuba тоже полный бред, ибо пример с «числами фибоначчи» высосан из пальца. Так будет делать только человек, не понимающий, как работает Eventloop и зачем он вообще нужен (что, кстати, доказывает пункт 1).  \r После доклада на DevConf этой весной, мне часто задают вопросы о том, стоит ли делать новый проект на NodeJS. Мой ответ всем:\r Если у вас куча времени и вы готовы проинвестировать его в развитие новой, сырой и спорной технологии — валяйте. Но если перед вами сроки/заказчики/инвесторы и у вас нет большого опыта с серверным JS за спиной — не стоит. \r Как показала практика, поднять проект на NodeJS — реально. Это работает. Но это стоило мне очень многого. Чего только стоит node open-source сообщество, о котором я еще постараюсь успеть написать.  Часть 2\r Вторая часть статьи будет на днях. Вот краткий список того, о чем я в ней напишу:\r 1. Поиск (elasticsearch)\r 2. Почта (google app engine)\r 3. Деплоймент (capistrano, npm)\r 4. Очереди (redis, kue)\r 5. COMET сервер (beseda) \r Слишком много информации для одной статьи.\r Если в комментариях увижу интересные вопросы, на них отвечу во второй части.  P.S.  Еды не будет. Любые комментарии, содержащие критику, без ссылки на собственные достижения буду игнорировать Мы ищем front-end ниндзя, детали здесь Напомню, что Tactoom — в закрытом beta-тестировании. Регистрация ограничена. Оставляйте email, и вам в скором времени, возможно, придет инвайт.  \r UPD 19.10:\r Вторая часть задерживается, ибо много работы. "
        }
    }
}